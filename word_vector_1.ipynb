{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis - Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding(\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print \"hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train=pd.read_csv(\"labeledTrainData.tsv\",header=0,delimiter=\"\\t\", quoting=3)\n",
    "test=pd.read_csv(\"testData.tsv\",header=0,delimiter=\"\\t\", quoting=3)\n",
    "unlabeled_train=pd.read_csv(\"unlabeledTrainData.tsv\",header=0,delimiter=\"\\t\",quoting=3)\n",
    "\n",
    "print train[\"review\"].size\n",
    "print test[\"review\"].size\n",
    "print unlabeled_train[\"review\"].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_wordlist(review,remove_stopwords=False):\n",
    "    review_text=BeautifulSoup(review).get_text()\n",
    "    text_only=re.sub(\"[^a-zA-Z]\",\" \",review_text)\n",
    "    words_lower=text_only.lower()\n",
    "    words=words_lower.split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops=set(stopwords.words(\"english\"))\n",
    "        words=[x for x in words if not x in stops]\n",
    "        \n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load punkt tokenizer\n",
    "tokenizer=nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "\n",
    "#splitting each review to sentences as Word2vec requires list of list\n",
    "\n",
    "def review_to_sentences(review,tokenizer,remove_stopwords=False):\n",
    "    \n",
    "    ## Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    \n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences=tokenizer.tokenize(review.decode(\"utf-8\").strip())  \n",
    "    # add decode to avoid unicode decode error later in the process.\n",
    "    \n",
    "    #loop over each sentence\n",
    "    sentences=[]\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence)>0:\n",
    "            sentence_wordlist=review_to_wordlist(raw_sentence,remove_stopwords)\n",
    "            sentences.append(sentence_wordlist)\n",
    "    \n",
    "    #this provides a list of lists of the entire reviews\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print \"hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences to training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\sentiment\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file C:\\Anaconda3\\envs\\sentiment\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html5lib\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "C:\\Anaconda3\\envs\\sentiment\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \".\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Anaconda3\\envs\\sentiment\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"...\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Anaconda3\\envs\\sentiment\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing sentences to unlabeled training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\sentiment\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Anaconda3\\envs\\sentiment\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Anaconda3\\envs\\sentiment\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"... ...\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Anaconda3\\envs\\sentiment\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"....\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Anaconda3\\envs\\sentiment\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Anaconda3\\envs\\sentiment\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Anaconda3\\envs\\sentiment\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Anaconda3\\envs\\sentiment\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \".. .\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Anaconda3\\envs\\sentiment\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "sentences=[]\n",
    "\n",
    "print \"Parsing sentences to training set\"\n",
    "\n",
    "\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review,tokenizer)\n",
    "        \n",
    "print \"parsing sentences to unlabeled training set\"\n",
    "\n",
    "\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences+= review_to_sentences(review,tokenizer)\n",
    "\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795538\n"
     ]
    }
   ],
   "source": [
    "print len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\sentiment\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2017-11-09 11:39:57,825 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2017-11-09 11:39:57,903 : INFO : collecting all words and their counts\n",
      "2017-11-09 11:39:57,903 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-11-09 11:39:57,934 : INFO : PROGRESS: at sentence #10000, processed 225803 words, keeping 17776 word types\n",
      "2017-11-09 11:39:57,980 : INFO : PROGRESS: at sentence #20000, processed 451892 words, keeping 24948 word types\n",
      "2017-11-09 11:39:58,013 : INFO : PROGRESS: at sentence #30000, processed 671315 words, keeping 30034 word types\n",
      "2017-11-09 11:39:58,065 : INFO : PROGRESS: at sentence #40000, processed 897815 words, keeping 34348 word types\n",
      "2017-11-09 11:39:58,098 : INFO : PROGRESS: at sentence #50000, processed 1116963 words, keeping 37761 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-09 11:39:58,144 : INFO : PROGRESS: at sentence #60000, processed 1338404 words, keeping 40723 word types\n",
      "2017-11-09 11:39:58,190 : INFO : PROGRESS: at sentence #70000, processed 1561580 words, keeping 43333 word types\n",
      "2017-11-09 11:39:58,229 : INFO : PROGRESS: at sentence #80000, processed 1780887 words, keeping 45714 word types\n",
      "2017-11-09 11:39:58,266 : INFO : PROGRESS: at sentence #90000, processed 2004996 words, keeping 48135 word types\n",
      "2017-11-09 11:39:58,305 : INFO : PROGRESS: at sentence #100000, processed 2226967 words, keeping 50207 word types\n",
      "2017-11-09 11:39:58,345 : INFO : PROGRESS: at sentence #110000, processed 2446581 words, keeping 52081 word types\n",
      "2017-11-09 11:39:58,385 : INFO : PROGRESS: at sentence #120000, processed 2668776 words, keeping 54119 word types\n",
      "2017-11-09 11:39:58,428 : INFO : PROGRESS: at sentence #130000, processed 2894304 words, keeping 55847 word types\n",
      "2017-11-09 11:39:58,460 : INFO : PROGRESS: at sentence #140000, processed 3107006 words, keeping 57346 word types\n",
      "2017-11-09 11:39:58,513 : INFO : PROGRESS: at sentence #150000, processed 3332628 words, keeping 59055 word types\n",
      "2017-11-09 11:39:58,545 : INFO : PROGRESS: at sentence #160000, processed 3555316 words, keeping 60617 word types\n",
      "2017-11-09 11:39:58,592 : INFO : PROGRESS: at sentence #170000, processed 3778656 words, keeping 62077 word types\n",
      "2017-11-09 11:39:58,648 : INFO : PROGRESS: at sentence #180000, processed 3999237 words, keeping 63496 word types\n",
      "2017-11-09 11:39:58,697 : INFO : PROGRESS: at sentence #190000, processed 4224450 words, keeping 64794 word types\n",
      "2017-11-09 11:39:58,753 : INFO : PROGRESS: at sentence #200000, processed 4448604 words, keeping 66087 word types\n",
      "2017-11-09 11:39:58,801 : INFO : PROGRESS: at sentence #210000, processed 4669968 words, keeping 67390 word types\n",
      "2017-11-09 11:39:58,852 : INFO : PROGRESS: at sentence #220000, processed 4894969 words, keeping 68697 word types\n",
      "2017-11-09 11:39:58,901 : INFO : PROGRESS: at sentence #230000, processed 5117546 words, keeping 69958 word types\n",
      "2017-11-09 11:39:58,948 : INFO : PROGRESS: at sentence #240000, processed 5345051 words, keeping 71167 word types\n",
      "2017-11-09 11:39:58,996 : INFO : PROGRESS: at sentence #250000, processed 5559166 words, keeping 72351 word types\n",
      "2017-11-09 11:39:59,045 : INFO : PROGRESS: at sentence #260000, processed 5779147 words, keeping 73478 word types\n",
      "2017-11-09 11:39:59,096 : INFO : PROGRESS: at sentence #270000, processed 6000436 words, keeping 74767 word types\n",
      "2017-11-09 11:39:59,150 : INFO : PROGRESS: at sentence #280000, processed 6226315 words, keeping 76369 word types\n",
      "2017-11-09 11:39:59,197 : INFO : PROGRESS: at sentence #290000, processed 6449475 words, keeping 77839 word types\n",
      "2017-11-09 11:39:59,250 : INFO : PROGRESS: at sentence #300000, processed 6674078 words, keeping 79171 word types\n",
      "2017-11-09 11:39:59,302 : INFO : PROGRESS: at sentence #310000, processed 6899392 words, keeping 80480 word types\n",
      "2017-11-09 11:39:59,349 : INFO : PROGRESS: at sentence #320000, processed 7124279 words, keeping 81808 word types\n",
      "2017-11-09 11:39:59,401 : INFO : PROGRESS: at sentence #330000, processed 7346022 words, keeping 83030 word types\n",
      "2017-11-09 11:39:59,453 : INFO : PROGRESS: at sentence #340000, processed 7575534 words, keeping 84280 word types\n",
      "2017-11-09 11:39:59,502 : INFO : PROGRESS: at sentence #350000, processed 7798804 words, keeping 85425 word types\n",
      "2017-11-09 11:39:59,549 : INFO : PROGRESS: at sentence #360000, processed 8019467 words, keeping 86596 word types\n",
      "2017-11-09 11:39:59,609 : INFO : PROGRESS: at sentence #370000, processed 8246659 words, keeping 87708 word types\n",
      "2017-11-09 11:39:59,665 : INFO : PROGRESS: at sentence #380000, processed 8471806 words, keeping 88878 word types\n",
      "2017-11-09 11:39:59,726 : INFO : PROGRESS: at sentence #390000, processed 8701556 words, keeping 89907 word types\n",
      "2017-11-09 11:39:59,778 : INFO : PROGRESS: at sentence #400000, processed 8924505 words, keeping 90916 word types\n",
      "2017-11-09 11:39:59,825 : INFO : PROGRESS: at sentence #410000, processed 9145855 words, keeping 91880 word types\n",
      "2017-11-09 11:39:59,878 : INFO : PROGRESS: at sentence #420000, processed 9366935 words, keeping 92912 word types\n",
      "2017-11-09 11:39:59,953 : INFO : PROGRESS: at sentence #430000, processed 9594472 words, keeping 93932 word types\n",
      "2017-11-09 11:40:00,009 : INFO : PROGRESS: at sentence #440000, processed 9821225 words, keeping 94906 word types\n",
      "2017-11-09 11:40:00,073 : INFO : PROGRESS: at sentence #450000, processed 10044987 words, keeping 96036 word types\n",
      "2017-11-09 11:40:00,128 : INFO : PROGRESS: at sentence #460000, processed 10277747 words, keeping 97088 word types\n",
      "2017-11-09 11:40:00,203 : INFO : PROGRESS: at sentence #470000, processed 10505672 words, keeping 97933 word types\n",
      "2017-11-09 11:40:00,267 : INFO : PROGRESS: at sentence #480000, processed 10726056 words, keeping 98862 word types\n",
      "2017-11-09 11:40:00,331 : INFO : PROGRESS: at sentence #490000, processed 10952800 words, keeping 99871 word types\n",
      "2017-11-09 11:40:00,392 : INFO : PROGRESS: at sentence #500000, processed 11174456 words, keeping 100765 word types\n",
      "2017-11-09 11:40:00,460 : INFO : PROGRESS: at sentence #510000, processed 11399731 words, keeping 101699 word types\n",
      "2017-11-09 11:40:00,506 : INFO : PROGRESS: at sentence #520000, processed 11623082 words, keeping 102598 word types\n",
      "2017-11-09 11:40:00,552 : INFO : PROGRESS: at sentence #530000, processed 11847480 words, keeping 103400 word types\n",
      "2017-11-09 11:40:00,605 : INFO : PROGRESS: at sentence #540000, processed 12072095 words, keeping 104265 word types\n",
      "2017-11-09 11:40:00,648 : INFO : PROGRESS: at sentence #550000, processed 12297646 words, keeping 105133 word types\n",
      "2017-11-09 11:40:00,697 : INFO : PROGRESS: at sentence #560000, processed 12518936 words, keeping 105997 word types\n",
      "2017-11-09 11:40:00,729 : INFO : PROGRESS: at sentence #570000, processed 12748083 words, keeping 106787 word types\n",
      "2017-11-09 11:40:00,785 : INFO : PROGRESS: at sentence #580000, processed 12969579 words, keeping 107665 word types\n",
      "2017-11-09 11:40:00,828 : INFO : PROGRESS: at sentence #590000, processed 13195104 words, keeping 108501 word types\n",
      "2017-11-09 11:40:00,875 : INFO : PROGRESS: at sentence #600000, processed 13417302 words, keeping 109218 word types\n",
      "2017-11-09 11:40:00,913 : INFO : PROGRESS: at sentence #610000, processed 13638325 words, keeping 110092 word types\n",
      "2017-11-09 11:40:00,960 : INFO : PROGRESS: at sentence #620000, processed 13864650 words, keeping 110837 word types\n",
      "2017-11-09 11:40:01,006 : INFO : PROGRESS: at sentence #630000, processed 14088936 words, keeping 111610 word types\n",
      "2017-11-09 11:40:01,038 : INFO : PROGRESS: at sentence #640000, processed 14309719 words, keeping 112416 word types\n",
      "2017-11-09 11:40:01,094 : INFO : PROGRESS: at sentence #650000, processed 14535475 words, keeping 113196 word types\n",
      "2017-11-09 11:40:01,128 : INFO : PROGRESS: at sentence #660000, processed 14758265 words, keeping 113945 word types\n",
      "2017-11-09 11:40:01,174 : INFO : PROGRESS: at sentence #670000, processed 14981658 words, keeping 114643 word types\n",
      "2017-11-09 11:40:01,217 : INFO : PROGRESS: at sentence #680000, processed 15206490 words, keeping 115354 word types\n",
      "2017-11-09 11:40:01,273 : INFO : PROGRESS: at sentence #690000, processed 15428683 words, keeping 116131 word types\n",
      "2017-11-09 11:40:01,319 : INFO : PROGRESS: at sentence #700000, processed 15657389 words, keeping 116943 word types\n",
      "2017-11-09 11:40:01,351 : INFO : PROGRESS: at sentence #710000, processed 15880378 words, keeping 117596 word types\n",
      "2017-11-09 11:40:01,410 : INFO : PROGRESS: at sentence #720000, processed 16105665 words, keeping 118221 word types\n",
      "2017-11-09 11:40:01,453 : INFO : PROGRESS: at sentence #730000, processed 16332046 words, keeping 118954 word types\n",
      "2017-11-09 11:40:01,500 : INFO : PROGRESS: at sentence #740000, processed 16553079 words, keeping 119668 word types\n",
      "2017-11-09 11:40:01,542 : INFO : PROGRESS: at sentence #750000, processed 16771406 words, keeping 120295 word types\n",
      "2017-11-09 11:40:01,585 : INFO : PROGRESS: at sentence #760000, processed 16990810 words, keeping 120930 word types\n",
      "2017-11-09 11:40:01,628 : INFO : PROGRESS: at sentence #770000, processed 17217947 words, keeping 121703 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-09 11:40:01,673 : INFO : PROGRESS: at sentence #780000, processed 17448093 words, keeping 122402 word types\n",
      "2017-11-09 11:40:01,720 : INFO : PROGRESS: at sentence #790000, processed 17675169 words, keeping 123066 word types\n",
      "2017-11-09 11:40:01,746 : INFO : collected 123504 word types from a corpus of 17798270 raw words and 795538 sentences\n",
      "2017-11-09 11:40:01,746 : INFO : Loading a fresh vocabulary\n",
      "2017-11-09 11:40:01,831 : INFO : min_count=40 retains 16490 unique words (13% of original 123504, drops 107014)\n",
      "2017-11-09 11:40:01,835 : INFO : min_count=40 leaves 17239125 word corpus (96% of original 17798270, drops 559145)\n",
      "2017-11-09 11:40:01,875 : INFO : deleting the raw counts dictionary of 123504 items\n",
      "2017-11-09 11:40:01,878 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2017-11-09 11:40:01,878 : INFO : downsampling leaves estimated 12749798 word corpus (74.0% of prior 17239125)\n",
      "2017-11-09 11:40:01,882 : INFO : estimated required memory for 16490 words and 300 dimensions: 47821000 bytes\n",
      "2017-11-09 11:40:01,930 : INFO : resetting layer weights\n",
      "2017-11-09 11:40:02,154 : INFO : training model with 6 workers on 16490 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-11-09 11:40:03,193 : INFO : PROGRESS: at 0.16% examples, 98867 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:40:04,217 : INFO : PROGRESS: at 0.37% examples, 115988 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:40:05,365 : INFO : PROGRESS: at 0.58% examples, 116731 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:40:06,417 : INFO : PROGRESS: at 0.82% examples, 124336 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:40:07,469 : INFO : PROGRESS: at 1.02% examples, 123773 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:40:08,437 : INFO : PROGRESS: at 1.26% examples, 127872 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:40:09,427 : INFO : PROGRESS: at 1.45% examples, 126963 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:40:10,443 : INFO : PROGRESS: at 1.66% examples, 127117 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:40:11,483 : INFO : PROGRESS: at 1.87% examples, 127818 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:40:12,506 : INFO : PROGRESS: at 2.08% examples, 128051 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:40:13,566 : INFO : PROGRESS: at 2.31% examples, 129186 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:40:14,607 : INFO : PROGRESS: at 2.55% examples, 130477 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:40:15,703 : INFO : PROGRESS: at 2.80% examples, 131282 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:40:16,707 : INFO : PROGRESS: at 3.06% examples, 133433 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:40:17,717 : INFO : PROGRESS: at 3.29% examples, 133940 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:40:18,845 : INFO : PROGRESS: at 3.53% examples, 133962 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:40:19,940 : INFO : PROGRESS: at 3.77% examples, 134668 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:40:21,029 : INFO : PROGRESS: at 3.99% examples, 134016 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:40:22,061 : INFO : PROGRESS: at 4.24% examples, 135022 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:40:23,121 : INFO : PROGRESS: at 4.46% examples, 134939 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:40:24,181 : INFO : PROGRESS: at 4.70% examples, 135192 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:40:25,217 : INFO : PROGRESS: at 4.93% examples, 135811 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-09 11:40:26,233 : INFO : PROGRESS: at 5.16% examples, 135899 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-09 11:40:27,361 : INFO : PROGRESS: at 5.38% examples, 135582 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:40:28,364 : INFO : PROGRESS: at 5.62% examples, 135981 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:40:29,453 : INFO : PROGRESS: at 5.86% examples, 136273 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:40:30,529 : INFO : PROGRESS: at 6.10% examples, 136500 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:40:31,546 : INFO : PROGRESS: at 6.36% examples, 137056 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:40:32,634 : INFO : PROGRESS: at 6.59% examples, 137182 words/s, in_qsize 10, out_qsize 1\n",
      "2017-11-09 11:40:33,746 : INFO : PROGRESS: at 6.82% examples, 136971 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:40:34,721 : INFO : PROGRESS: at 7.05% examples, 137282 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:40:35,778 : INFO : PROGRESS: at 7.26% examples, 136943 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:40:36,813 : INFO : PROGRESS: at 7.46% examples, 136488 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:40:37,914 : INFO : PROGRESS: at 7.67% examples, 136115 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:40:38,934 : INFO : PROGRESS: at 7.94% examples, 137012 words/s, in_qsize 10, out_qsize 1\n",
      "2017-11-09 11:40:39,950 : INFO : PROGRESS: at 8.18% examples, 137422 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:40:40,993 : INFO : PROGRESS: at 8.40% examples, 137371 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:40:42,013 : INFO : PROGRESS: at 8.63% examples, 137543 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-09 11:40:43,082 : INFO : PROGRESS: at 8.83% examples, 137171 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:40:44,095 : INFO : PROGRESS: at 9.06% examples, 137126 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:40:45,142 : INFO : PROGRESS: at 9.29% examples, 137428 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:40:46,207 : INFO : PROGRESS: at 9.55% examples, 137741 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:40:47,250 : INFO : PROGRESS: at 9.77% examples, 137836 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:40:48,315 : INFO : PROGRESS: at 9.99% examples, 137749 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:40:49,302 : INFO : PROGRESS: at 10.20% examples, 137608 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:40:50,331 : INFO : PROGRESS: at 10.44% examples, 137834 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:40:51,382 : INFO : PROGRESS: at 10.65% examples, 137699 words/s, in_qsize 9, out_qsize 1\n",
      "2017-11-09 11:40:52,391 : INFO : PROGRESS: at 10.86% examples, 137550 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:40:53,418 : INFO : PROGRESS: at 11.10% examples, 137723 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:40:54,515 : INFO : PROGRESS: at 11.33% examples, 137844 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:40:55,510 : INFO : PROGRESS: at 11.55% examples, 137863 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:40:56,555 : INFO : PROGRESS: at 11.78% examples, 137928 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:40:57,591 : INFO : PROGRESS: at 11.99% examples, 137857 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:40:58,602 : INFO : PROGRESS: at 12.22% examples, 137895 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:40:59,690 : INFO : PROGRESS: at 12.45% examples, 137945 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-09 11:41:00,678 : INFO : PROGRESS: at 12.67% examples, 137867 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:41:01,703 : INFO : PROGRESS: at 12.89% examples, 137931 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:41:02,802 : INFO : PROGRESS: at 13.10% examples, 137729 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:41:03,835 : INFO : PROGRESS: at 13.36% examples, 138034 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:41:04,914 : INFO : PROGRESS: at 13.59% examples, 138115 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:41:05,887 : INFO : PROGRESS: at 13.81% examples, 138069 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-09 11:41:06,934 : INFO : PROGRESS: at 14.01% examples, 137823 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:41:07,930 : INFO : PROGRESS: at 14.25% examples, 138119 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:41:08,947 : INFO : PROGRESS: at 14.51% examples, 138510 words/s, in_qsize 11, out_qsize 1\n",
      "2017-11-09 11:41:09,986 : INFO : PROGRESS: at 14.72% examples, 138443 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:41:10,983 : INFO : PROGRESS: at 14.95% examples, 138426 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:41:12,026 : INFO : PROGRESS: at 15.20% examples, 138627 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:41:13,154 : INFO : PROGRESS: at 15.46% examples, 138839 words/s, in_qsize 12, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-09 11:41:14,178 : INFO : PROGRESS: at 15.72% examples, 139136 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:41:15,178 : INFO : PROGRESS: at 15.94% examples, 139082 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:41:16,243 : INFO : PROGRESS: at 16.16% examples, 139067 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:41:17,257 : INFO : PROGRESS: at 16.37% examples, 138986 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:41:18,267 : INFO : PROGRESS: at 16.60% examples, 138991 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:41:19,263 : INFO : PROGRESS: at 16.80% examples, 138857 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:41:20,292 : INFO : PROGRESS: at 17.01% examples, 138809 words/s, in_qsize 10, out_qsize 1\n",
      "2017-11-09 11:41:21,384 : INFO : PROGRESS: at 17.26% examples, 138848 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:41:22,460 : INFO : PROGRESS: at 17.48% examples, 138772 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:41:23,487 : INFO : PROGRESS: at 17.70% examples, 138787 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:41:24,523 : INFO : PROGRESS: at 17.97% examples, 139179 words/s, in_qsize 10, out_qsize 1\n",
      "2017-11-09 11:41:25,548 : INFO : PROGRESS: at 18.19% examples, 139063 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:41:26,648 : INFO : PROGRESS: at 18.39% examples, 138839 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:41:27,655 : INFO : PROGRESS: at 18.59% examples, 138614 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:41:28,803 : INFO : PROGRESS: at 18.85% examples, 138667 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:41:29,891 : INFO : PROGRESS: at 19.10% examples, 138702 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:41:30,936 : INFO : PROGRESS: at 19.33% examples, 138723 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:41:32,043 : INFO : PROGRESS: at 19.57% examples, 138833 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:41:33,043 : INFO : PROGRESS: at 19.83% examples, 139142 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:41:34,043 : INFO : PROGRESS: at 20.10% examples, 139486 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:41:35,131 : INFO : PROGRESS: at 20.30% examples, 139277 words/s, in_qsize 10, out_qsize 1\n",
      "2017-11-09 11:41:36,124 : INFO : PROGRESS: at 20.49% examples, 139062 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:41:37,216 : INFO : PROGRESS: at 20.69% examples, 138794 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:41:38,187 : INFO : PROGRESS: at 20.89% examples, 138680 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:41:39,220 : INFO : PROGRESS: at 21.05% examples, 138352 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:41:40,213 : INFO : PROGRESS: at 21.27% examples, 138290 words/s, in_qsize 10, out_qsize 1\n",
      "2017-11-09 11:41:41,256 : INFO : PROGRESS: at 21.49% examples, 138208 words/s, in_qsize 10, out_qsize 1\n",
      "2017-11-09 11:41:42,269 : INFO : PROGRESS: at 21.67% examples, 137946 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:41:43,332 : INFO : PROGRESS: at 21.93% examples, 138127 words/s, in_qsize 10, out_qsize 1\n",
      "2017-11-09 11:41:44,413 : INFO : PROGRESS: at 22.14% examples, 138038 words/s, in_qsize 11, out_qsize 1\n",
      "2017-11-09 11:41:45,473 : INFO : PROGRESS: at 22.37% examples, 138038 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:41:46,453 : INFO : PROGRESS: at 22.60% examples, 138076 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:41:47,525 : INFO : PROGRESS: at 22.82% examples, 138072 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:41:48,484 : INFO : PROGRESS: at 23.04% examples, 138046 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:41:49,516 : INFO : PROGRESS: at 23.24% examples, 137950 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:41:50,565 : INFO : PROGRESS: at 23.48% examples, 138020 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-09 11:41:51,601 : INFO : PROGRESS: at 23.68% examples, 137824 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:41:52,628 : INFO : PROGRESS: at 23.93% examples, 137968 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:41:53,688 : INFO : PROGRESS: at 24.16% examples, 137929 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:41:54,812 : INFO : PROGRESS: at 24.41% examples, 137968 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:41:55,801 : INFO : PROGRESS: at 24.63% examples, 138013 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:41:56,861 : INFO : PROGRESS: at 24.88% examples, 138119 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:41:57,948 : INFO : PROGRESS: at 25.11% examples, 138181 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:41:58,953 : INFO : PROGRESS: at 25.31% examples, 138081 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-09 11:41:59,948 : INFO : PROGRESS: at 25.54% examples, 138086 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:42:01,000 : INFO : PROGRESS: at 25.80% examples, 138256 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:42:02,056 : INFO : PROGRESS: at 26.03% examples, 138258 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:42:03,144 : INFO : PROGRESS: at 26.28% examples, 138278 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:42:04,197 : INFO : PROGRESS: at 26.52% examples, 138334 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:42:05,184 : INFO : PROGRESS: at 26.77% examples, 138464 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:42:06,217 : INFO : PROGRESS: at 26.94% examples, 138250 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:42:07,220 : INFO : PROGRESS: at 27.19% examples, 138398 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:42:08,276 : INFO : PROGRESS: at 27.39% examples, 138244 words/s, in_qsize 7, out_qsize 2\n",
      "2017-11-09 11:42:09,289 : INFO : PROGRESS: at 27.61% examples, 138272 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:42:10,417 : INFO : PROGRESS: at 27.91% examples, 138562 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:42:11,453 : INFO : PROGRESS: at 28.14% examples, 138577 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:42:12,464 : INFO : PROGRESS: at 28.35% examples, 138538 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:42:13,540 : INFO : PROGRESS: at 28.57% examples, 138494 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:42:14,540 : INFO : PROGRESS: at 28.83% examples, 138669 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:42:15,608 : INFO : PROGRESS: at 29.05% examples, 138640 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:42:16,621 : INFO : PROGRESS: at 29.28% examples, 138714 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:42:17,581 : INFO : PROGRESS: at 29.47% examples, 138586 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-09 11:42:18,631 : INFO : PROGRESS: at 29.69% examples, 138568 words/s, in_qsize 9, out_qsize 0\n",
      "2017-11-09 11:42:19,676 : INFO : PROGRESS: at 29.91% examples, 138536 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:42:20,716 : INFO : PROGRESS: at 30.15% examples, 138613 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:42:21,732 : INFO : PROGRESS: at 30.36% examples, 138590 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:42:22,767 : INFO : PROGRESS: at 30.60% examples, 138612 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:42:23,759 : INFO : PROGRESS: at 30.84% examples, 138734 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:42:24,792 : INFO : PROGRESS: at 31.06% examples, 138750 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:42:25,859 : INFO : PROGRESS: at 31.29% examples, 138732 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:42:26,823 : INFO : PROGRESS: at 31.52% examples, 138804 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-09 11:42:27,875 : INFO : PROGRESS: at 31.69% examples, 138614 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:42:28,907 : INFO : PROGRESS: at 31.91% examples, 138622 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:42:29,947 : INFO : PROGRESS: at 32.14% examples, 138610 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-09 11:42:30,980 : INFO : PROGRESS: at 32.36% examples, 138613 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:42:32,012 : INFO : PROGRESS: at 32.57% examples, 138517 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:42:33,039 : INFO : PROGRESS: at 32.80% examples, 138537 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:42:34,240 : INFO : PROGRESS: at 33.07% examples, 138569 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:42:35,247 : INFO : PROGRESS: at 33.30% examples, 138643 words/s, in_qsize 11, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-09 11:42:36,272 : INFO : PROGRESS: at 33.54% examples, 138690 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:42:37,312 : INFO : PROGRESS: at 33.79% examples, 138775 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:42:38,364 : INFO : PROGRESS: at 33.99% examples, 138691 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:42:39,375 : INFO : PROGRESS: at 34.22% examples, 138727 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:42:40,555 : INFO : PROGRESS: at 34.41% examples, 138461 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:42:41,592 : INFO : PROGRESS: at 34.62% examples, 138415 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:42:42,640 : INFO : PROGRESS: at 34.85% examples, 138401 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:42:43,691 : INFO : PROGRESS: at 35.08% examples, 138438 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:42:44,743 : INFO : PROGRESS: at 35.32% examples, 138461 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:42:45,772 : INFO : PROGRESS: at 35.53% examples, 138441 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:42:46,823 : INFO : PROGRESS: at 35.76% examples, 138407 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:42:47,839 : INFO : PROGRESS: at 35.97% examples, 138377 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:42:48,875 : INFO : PROGRESS: at 36.15% examples, 138205 words/s, in_qsize 10, out_qsize 2\n",
      "2017-11-09 11:42:49,880 : INFO : PROGRESS: at 36.43% examples, 138442 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:42:50,900 : INFO : PROGRESS: at 36.65% examples, 138418 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:42:52,039 : INFO : PROGRESS: at 36.85% examples, 138276 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:42:53,023 : INFO : PROGRESS: at 37.09% examples, 138369 words/s, in_qsize 9, out_qsize 0\n",
      "2017-11-09 11:42:54,063 : INFO : PROGRESS: at 37.32% examples, 138378 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:42:55,099 : INFO : PROGRESS: at 37.58% examples, 138526 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:42:56,140 : INFO : PROGRESS: at 37.83% examples, 138627 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:42:57,155 : INFO : PROGRESS: at 38.06% examples, 138655 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:42:58,184 : INFO : PROGRESS: at 38.28% examples, 138617 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:42:59,227 : INFO : PROGRESS: at 38.51% examples, 138668 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:43:00,250 : INFO : PROGRESS: at 38.74% examples, 138676 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-09 11:43:01,282 : INFO : PROGRESS: at 38.97% examples, 138658 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:43:02,309 : INFO : PROGRESS: at 39.18% examples, 138641 words/s, in_qsize 9, out_qsize 0\n",
      "2017-11-09 11:43:03,361 : INFO : PROGRESS: at 39.41% examples, 138663 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:43:04,374 : INFO : PROGRESS: at 39.63% examples, 138680 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:43:05,361 : INFO : PROGRESS: at 39.90% examples, 138851 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:43:06,378 : INFO : PROGRESS: at 40.11% examples, 138824 words/s, in_qsize 7, out_qsize 1\n",
      "2017-11-09 11:43:07,405 : INFO : PROGRESS: at 40.30% examples, 138719 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:43:08,410 : INFO : PROGRESS: at 40.55% examples, 138810 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:43:09,434 : INFO : PROGRESS: at 40.72% examples, 138620 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:43:10,446 : INFO : PROGRESS: at 40.93% examples, 138595 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:43:11,482 : INFO : PROGRESS: at 41.11% examples, 138466 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:43:12,497 : INFO : PROGRESS: at 41.30% examples, 138351 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:43:13,630 : INFO : PROGRESS: at 41.53% examples, 138266 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:43:14,701 : INFO : PROGRESS: at 41.77% examples, 138279 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:43:15,710 : INFO : PROGRESS: at 42.00% examples, 138302 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:43:16,710 : INFO : PROGRESS: at 42.22% examples, 138321 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:43:17,861 : INFO : PROGRESS: at 42.46% examples, 138314 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:43:18,842 : INFO : PROGRESS: at 42.70% examples, 138364 words/s, in_qsize 9, out_qsize 0\n",
      "2017-11-09 11:43:19,842 : INFO : PROGRESS: at 42.95% examples, 138446 words/s, in_qsize 9, out_qsize 0\n",
      "2017-11-09 11:43:20,966 : INFO : PROGRESS: at 43.19% examples, 138512 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:43:21,910 : INFO : PROGRESS: at 43.46% examples, 138626 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:43:22,982 : INFO : PROGRESS: at 43.68% examples, 138628 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:43:23,970 : INFO : PROGRESS: at 43.90% examples, 138592 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:43:25,009 : INFO : PROGRESS: at 44.13% examples, 138587 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:43:26,141 : INFO : PROGRESS: at 44.35% examples, 138552 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:43:27,161 : INFO : PROGRESS: at 44.56% examples, 138525 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:43:28,125 : INFO : PROGRESS: at 44.79% examples, 138541 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:43:29,147 : INFO : PROGRESS: at 45.01% examples, 138560 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:43:30,203 : INFO : PROGRESS: at 45.24% examples, 138559 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:43:31,194 : INFO : PROGRESS: at 45.45% examples, 138521 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:43:32,295 : INFO : PROGRESS: at 45.71% examples, 138595 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:43:33,362 : INFO : PROGRESS: at 45.93% examples, 138555 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:43:34,391 : INFO : PROGRESS: at 46.21% examples, 138692 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:43:35,483 : INFO : PROGRESS: at 46.44% examples, 138694 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:43:36,430 : INFO : PROGRESS: at 46.65% examples, 138673 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:43:37,523 : INFO : PROGRESS: at 46.88% examples, 138648 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:43:38,526 : INFO : PROGRESS: at 47.11% examples, 138681 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:43:39,571 : INFO : PROGRESS: at 47.36% examples, 138739 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:43:40,647 : INFO : PROGRESS: at 47.63% examples, 138847 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:43:41,667 : INFO : PROGRESS: at 47.87% examples, 138928 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:43:42,710 : INFO : PROGRESS: at 48.08% examples, 138874 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:43:43,746 : INFO : PROGRESS: at 48.35% examples, 139000 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:43:44,779 : INFO : PROGRESS: at 48.57% examples, 139002 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:43:45,799 : INFO : PROGRESS: at 48.81% examples, 139038 words/s, in_qsize 10, out_qsize 1\n",
      "2017-11-09 11:43:46,846 : INFO : PROGRESS: at 49.06% examples, 139105 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:43:47,891 : INFO : PROGRESS: at 49.28% examples, 139090 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:43:48,947 : INFO : PROGRESS: at 49.50% examples, 139075 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:43:49,983 : INFO : PROGRESS: at 49.73% examples, 139085 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:43:50,986 : INFO : PROGRESS: at 49.95% examples, 139099 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:43:52,039 : INFO : PROGRESS: at 50.18% examples, 139087 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:43:53,111 : INFO : PROGRESS: at 50.42% examples, 139105 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:43:54,098 : INFO : PROGRESS: at 50.65% examples, 139143 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:43:55,154 : INFO : PROGRESS: at 50.91% examples, 139218 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:43:56,219 : INFO : PROGRESS: at 51.16% examples, 139277 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:43:57,266 : INFO : PROGRESS: at 51.39% examples, 139313 words/s, in_qsize 11, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-09 11:43:58,239 : INFO : PROGRESS: at 51.64% examples, 139412 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:43:59,239 : INFO : PROGRESS: at 51.88% examples, 139480 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:44:00,263 : INFO : PROGRESS: at 52.11% examples, 139472 words/s, in_qsize 9, out_qsize 1\n",
      "2017-11-09 11:44:01,302 : INFO : PROGRESS: at 52.38% examples, 139622 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-09 11:44:02,367 : INFO : PROGRESS: at 52.61% examples, 139606 words/s, in_qsize 12, out_qsize 1\n",
      "2017-11-09 11:44:03,391 : INFO : PROGRESS: at 52.85% examples, 139627 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:44:04,398 : INFO : PROGRESS: at 53.14% examples, 139813 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:44:05,503 : INFO : PROGRESS: at 53.37% examples, 139790 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:44:06,555 : INFO : PROGRESS: at 53.62% examples, 139831 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:44:07,622 : INFO : PROGRESS: at 53.83% examples, 139799 words/s, in_qsize 10, out_qsize 1\n",
      "2017-11-09 11:44:08,615 : INFO : PROGRESS: at 54.07% examples, 139838 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:44:09,642 : INFO : PROGRESS: at 54.29% examples, 139820 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:44:10,678 : INFO : PROGRESS: at 54.54% examples, 139914 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:44:11,690 : INFO : PROGRESS: at 54.76% examples, 139891 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:44:12,678 : INFO : PROGRESS: at 54.97% examples, 139870 words/s, in_qsize 10, out_qsize 1\n",
      "2017-11-09 11:44:13,690 : INFO : PROGRESS: at 55.23% examples, 139960 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:44:14,790 : INFO : PROGRESS: at 55.43% examples, 139894 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:44:15,786 : INFO : PROGRESS: at 55.66% examples, 139926 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:44:16,770 : INFO : PROGRESS: at 55.89% examples, 139936 words/s, in_qsize 8, out_qsize 1\n",
      "2017-11-09 11:44:17,907 : INFO : PROGRESS: at 56.13% examples, 139919 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:44:18,867 : INFO : PROGRESS: at 56.38% examples, 140008 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:44:19,898 : INFO : PROGRESS: at 56.66% examples, 140118 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:44:20,898 : INFO : PROGRESS: at 56.90% examples, 140180 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:44:21,923 : INFO : PROGRESS: at 57.15% examples, 140242 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:44:22,983 : INFO : PROGRESS: at 57.36% examples, 140184 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:44:24,023 : INFO : PROGRESS: at 57.57% examples, 140160 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:44:25,095 : INFO : PROGRESS: at 57.81% examples, 140168 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:44:26,131 : INFO : PROGRESS: at 58.02% examples, 140136 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:44:27,210 : INFO : PROGRESS: at 58.26% examples, 140159 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:44:28,214 : INFO : PROGRESS: at 58.48% examples, 140132 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:44:29,246 : INFO : PROGRESS: at 58.70% examples, 140122 words/s, in_qsize 9, out_qsize 0\n",
      "2017-11-09 11:44:30,311 : INFO : PROGRESS: at 58.94% examples, 140135 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:44:31,338 : INFO : PROGRESS: at 59.18% examples, 140136 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:44:32,391 : INFO : PROGRESS: at 59.41% examples, 140171 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:44:33,382 : INFO : PROGRESS: at 59.63% examples, 140175 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-09 11:44:34,382 : INFO : PROGRESS: at 59.85% examples, 140185 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-09 11:44:35,438 : INFO : PROGRESS: at 60.08% examples, 140154 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:44:36,474 : INFO : PROGRESS: at 60.31% examples, 140183 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:44:37,494 : INFO : PROGRESS: at 60.55% examples, 140213 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:44:38,542 : INFO : PROGRESS: at 60.79% examples, 140220 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:44:39,566 : INFO : PROGRESS: at 61.01% examples, 140218 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:44:40,551 : INFO : PROGRESS: at 61.25% examples, 140250 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:44:41,598 : INFO : PROGRESS: at 61.47% examples, 140235 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:44:42,671 : INFO : PROGRESS: at 61.72% examples, 140279 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:44:43,739 : INFO : PROGRESS: at 61.96% examples, 140267 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:44:44,763 : INFO : PROGRESS: at 62.19% examples, 140271 words/s, in_qsize 10, out_qsize 1\n",
      "2017-11-09 11:44:45,806 : INFO : PROGRESS: at 62.42% examples, 140284 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:44:46,811 : INFO : PROGRESS: at 62.66% examples, 140309 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:44:47,846 : INFO : PROGRESS: at 62.88% examples, 140289 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:44:48,871 : INFO : PROGRESS: at 63.11% examples, 140311 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:44:49,882 : INFO : PROGRESS: at 63.37% examples, 140353 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:44:50,938 : INFO : PROGRESS: at 63.61% examples, 140384 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-09 11:44:51,917 : INFO : PROGRESS: at 63.83% examples, 140392 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-09 11:44:52,937 : INFO : PROGRESS: at 64.06% examples, 140392 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:44:53,989 : INFO : PROGRESS: at 64.28% examples, 140373 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:44:55,036 : INFO : PROGRESS: at 64.53% examples, 140398 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:44:56,069 : INFO : PROGRESS: at 64.75% examples, 140392 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:44:57,137 : INFO : PROGRESS: at 64.99% examples, 140409 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:44:58,161 : INFO : PROGRESS: at 65.22% examples, 140415 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:44:59,121 : INFO : PROGRESS: at 65.45% examples, 140441 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:45:00,209 : INFO : PROGRESS: at 65.67% examples, 140429 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:45:01,217 : INFO : PROGRESS: at 65.93% examples, 140498 words/s, in_qsize 9, out_qsize 1\n",
      "2017-11-09 11:45:02,220 : INFO : PROGRESS: at 66.17% examples, 140527 words/s, in_qsize 9, out_qsize 0\n",
      "2017-11-09 11:45:03,273 : INFO : PROGRESS: at 66.39% examples, 140477 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:45:04,301 : INFO : PROGRESS: at 66.64% examples, 140518 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:45:05,305 : INFO : PROGRESS: at 66.88% examples, 140548 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:45:06,305 : INFO : PROGRESS: at 67.10% examples, 140555 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:45:07,312 : INFO : PROGRESS: at 67.32% examples, 140559 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:45:08,365 : INFO : PROGRESS: at 67.55% examples, 140561 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:45:09,377 : INFO : PROGRESS: at 67.78% examples, 140588 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:45:10,384 : INFO : PROGRESS: at 68.01% examples, 140594 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:45:11,413 : INFO : PROGRESS: at 68.23% examples, 140582 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:45:12,400 : INFO : PROGRESS: at 68.46% examples, 140610 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:45:13,407 : INFO : PROGRESS: at 68.67% examples, 140588 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:45:14,483 : INFO : PROGRESS: at 68.92% examples, 140624 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:45:15,487 : INFO : PROGRESS: at 69.14% examples, 140624 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:45:16,503 : INFO : PROGRESS: at 69.38% examples, 140648 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:45:17,532 : INFO : PROGRESS: at 69.60% examples, 140649 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:45:18,503 : INFO : PROGRESS: at 69.82% examples, 140652 words/s, in_qsize 10, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-09 11:45:19,555 : INFO : PROGRESS: at 70.04% examples, 140648 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:45:20,568 : INFO : PROGRESS: at 70.28% examples, 140671 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-09 11:45:21,599 : INFO : PROGRESS: at 70.51% examples, 140664 words/s, in_qsize 9, out_qsize 0\n",
      "2017-11-09 11:45:22,615 : INFO : PROGRESS: at 70.73% examples, 140662 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:45:23,655 : INFO : PROGRESS: at 70.95% examples, 140650 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:45:24,720 : INFO : PROGRESS: at 71.18% examples, 140641 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:45:25,680 : INFO : PROGRESS: at 71.41% examples, 140668 words/s, in_qsize 9, out_qsize 2\n",
      "2017-11-09 11:45:26,687 : INFO : PROGRESS: at 71.68% examples, 140781 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:45:27,818 : INFO : PROGRESS: at 71.97% examples, 140865 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:45:28,894 : INFO : PROGRESS: at 72.20% examples, 140874 words/s, in_qsize 11, out_qsize 1\n",
      "2017-11-09 11:45:29,914 : INFO : PROGRESS: at 72.47% examples, 140947 words/s, in_qsize 9, out_qsize 1\n",
      "2017-11-09 11:45:30,973 : INFO : PROGRESS: at 72.71% examples, 140954 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:45:31,982 : INFO : PROGRESS: at 72.95% examples, 140974 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:45:33,065 : INFO : PROGRESS: at 73.17% examples, 140963 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:45:34,026 : INFO : PROGRESS: at 73.38% examples, 140947 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:45:35,121 : INFO : PROGRESS: at 73.63% examples, 140955 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:45:36,190 : INFO : PROGRESS: at 73.85% examples, 140931 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:45:37,206 : INFO : PROGRESS: at 74.09% examples, 140954 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:45:38,269 : INFO : PROGRESS: at 74.32% examples, 140965 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:45:39,358 : INFO : PROGRESS: at 74.56% examples, 140947 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:45:40,361 : INFO : PROGRESS: at 74.77% examples, 140931 words/s, in_qsize 11, out_qsize 1\n",
      "2017-11-09 11:45:41,405 : INFO : PROGRESS: at 75.00% examples, 140954 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:45:42,421 : INFO : PROGRESS: at 75.23% examples, 140948 words/s, in_qsize 9, out_qsize 0\n",
      "2017-11-09 11:45:43,398 : INFO : PROGRESS: at 75.44% examples, 140933 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:45:44,437 : INFO : PROGRESS: at 75.67% examples, 140944 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:45:45,497 : INFO : PROGRESS: at 75.90% examples, 140938 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:45:46,509 : INFO : PROGRESS: at 76.13% examples, 140936 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-09 11:45:47,502 : INFO : PROGRESS: at 76.36% examples, 140957 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:45:48,598 : INFO : PROGRESS: at 76.59% examples, 140938 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:45:49,614 : INFO : PROGRESS: at 76.82% examples, 140955 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:45:50,609 : INFO : PROGRESS: at 77.06% examples, 140971 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:45:51,661 : INFO : PROGRESS: at 77.28% examples, 140975 words/s, in_qsize 11, out_qsize 1\n",
      "2017-11-09 11:45:52,650 : INFO : PROGRESS: at 77.54% examples, 141033 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:45:53,661 : INFO : PROGRESS: at 77.75% examples, 141009 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:45:54,717 : INFO : PROGRESS: at 77.96% examples, 140986 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:45:55,730 : INFO : PROGRESS: at 78.20% examples, 141003 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:45:56,740 : INFO : PROGRESS: at 78.43% examples, 141016 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:45:57,891 : INFO : PROGRESS: at 78.68% examples, 141019 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:45:59,000 : INFO : PROGRESS: at 78.93% examples, 141012 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:00,009 : INFO : PROGRESS: at 79.19% examples, 141067 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:01,045 : INFO : PROGRESS: at 79.42% examples, 141078 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:46:02,132 : INFO : PROGRESS: at 79.67% examples, 141090 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:46:03,213 : INFO : PROGRESS: at 79.91% examples, 141114 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-09 11:46:04,256 : INFO : PROGRESS: at 80.12% examples, 141085 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:05,253 : INFO : PROGRESS: at 80.33% examples, 141068 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:06,292 : INFO : PROGRESS: at 80.56% examples, 141064 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:07,280 : INFO : PROGRESS: at 80.79% examples, 141060 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:46:08,328 : INFO : PROGRESS: at 81.01% examples, 141054 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:09,361 : INFO : PROGRESS: at 81.24% examples, 141047 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:46:10,365 : INFO : PROGRESS: at 81.47% examples, 141071 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:11,401 : INFO : PROGRESS: at 81.70% examples, 141069 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:12,413 : INFO : PROGRESS: at 81.94% examples, 141082 words/s, in_qsize 9, out_qsize 1\n",
      "2017-11-09 11:46:13,417 : INFO : PROGRESS: at 82.17% examples, 141095 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:14,484 : INFO : PROGRESS: at 82.41% examples, 141103 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:46:15,532 : INFO : PROGRESS: at 82.64% examples, 141099 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:46:16,561 : INFO : PROGRESS: at 82.88% examples, 141115 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:17,546 : INFO : PROGRESS: at 83.17% examples, 141222 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:18,594 : INFO : PROGRESS: at 83.45% examples, 141285 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:19,697 : INFO : PROGRESS: at 83.72% examples, 141332 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:20,710 : INFO : PROGRESS: at 83.95% examples, 141351 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:21,757 : INFO : PROGRESS: at 84.20% examples, 141366 words/s, in_qsize 10, out_qsize 2\n",
      "2017-11-09 11:46:22,786 : INFO : PROGRESS: at 84.48% examples, 141436 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:23,829 : INFO : PROGRESS: at 84.70% examples, 141439 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:46:24,809 : INFO : PROGRESS: at 84.93% examples, 141456 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:46:25,937 : INFO : PROGRESS: at 85.18% examples, 141471 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:46:26,898 : INFO : PROGRESS: at 85.43% examples, 141504 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:46:27,966 : INFO : PROGRESS: at 85.67% examples, 141521 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:28,993 : INFO : PROGRESS: at 85.91% examples, 141541 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:30,017 : INFO : PROGRESS: at 86.14% examples, 141540 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:46:31,062 : INFO : PROGRESS: at 86.38% examples, 141547 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:32,049 : INFO : PROGRESS: at 86.62% examples, 141560 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:46:33,094 : INFO : PROGRESS: at 86.84% examples, 141556 words/s, in_qsize 10, out_qsize 1\n",
      "2017-11-09 11:46:34,130 : INFO : PROGRESS: at 87.07% examples, 141553 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:46:35,125 : INFO : PROGRESS: at 87.31% examples, 141579 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:36,130 : INFO : PROGRESS: at 87.55% examples, 141600 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:46:37,181 : INFO : PROGRESS: at 87.78% examples, 141613 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:46:38,242 : INFO : PROGRESS: at 88.00% examples, 141605 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:39,246 : INFO : PROGRESS: at 88.24% examples, 141614 words/s, in_qsize 9, out_qsize 0\n",
      "2017-11-09 11:46:40,269 : INFO : PROGRESS: at 88.46% examples, 141613 words/s, in_qsize 11, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-09 11:46:41,242 : INFO : PROGRESS: at 88.68% examples, 141615 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:46:42,266 : INFO : PROGRESS: at 88.91% examples, 141619 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:43,309 : INFO : PROGRESS: at 89.13% examples, 141607 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:44,338 : INFO : PROGRESS: at 89.36% examples, 141598 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:45,378 : INFO : PROGRESS: at 89.60% examples, 141631 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:46,369 : INFO : PROGRESS: at 89.82% examples, 141631 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:47,394 : INFO : PROGRESS: at 90.04% examples, 141626 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:48,421 : INFO : PROGRESS: at 90.28% examples, 141629 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:49,493 : INFO : PROGRESS: at 90.51% examples, 141623 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:50,453 : INFO : PROGRESS: at 90.74% examples, 141643 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:51,497 : INFO : PROGRESS: at 90.96% examples, 141636 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:52,614 : INFO : PROGRESS: at 91.21% examples, 141640 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:46:53,605 : INFO : PROGRESS: at 91.45% examples, 141668 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:46:54,638 : INFO : PROGRESS: at 91.66% examples, 141643 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:46:55,690 : INFO : PROGRESS: at 91.89% examples, 141659 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:46:56,706 : INFO : PROGRESS: at 92.14% examples, 141677 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:46:57,757 : INFO : PROGRESS: at 92.38% examples, 141706 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:46:58,766 : INFO : PROGRESS: at 92.61% examples, 141699 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:46:59,802 : INFO : PROGRESS: at 92.83% examples, 141689 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:47:00,834 : INFO : PROGRESS: at 93.07% examples, 141706 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:47:01,858 : INFO : PROGRESS: at 93.30% examples, 141721 words/s, in_qsize 9, out_qsize 0\n",
      "2017-11-09 11:47:02,874 : INFO : PROGRESS: at 93.54% examples, 141725 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:47:03,953 : INFO : PROGRESS: at 93.76% examples, 141703 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:47:04,930 : INFO : PROGRESS: at 94.00% examples, 141736 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:47:05,914 : INFO : PROGRESS: at 94.23% examples, 141756 words/s, in_qsize 9, out_qsize 2\n",
      "2017-11-09 11:47:06,973 : INFO : PROGRESS: at 94.50% examples, 141807 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:47:08,025 : INFO : PROGRESS: at 94.74% examples, 141810 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:47:09,085 : INFO : PROGRESS: at 94.97% examples, 141809 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:47:10,137 : INFO : PROGRESS: at 95.21% examples, 141823 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:47:11,164 : INFO : PROGRESS: at 95.43% examples, 141812 words/s, in_qsize 10, out_qsize 0\n",
      "2017-11-09 11:47:12,148 : INFO : PROGRESS: at 95.65% examples, 141813 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:47:13,173 : INFO : PROGRESS: at 95.88% examples, 141814 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:47:14,200 : INFO : PROGRESS: at 96.11% examples, 141809 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:47:15,213 : INFO : PROGRESS: at 96.33% examples, 141808 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:47:16,224 : INFO : PROGRESS: at 96.56% examples, 141807 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-09 11:47:17,240 : INFO : PROGRESS: at 96.78% examples, 141804 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:47:18,224 : INFO : PROGRESS: at 97.00% examples, 141803 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:47:19,269 : INFO : PROGRESS: at 97.23% examples, 141802 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:47:20,256 : INFO : PROGRESS: at 97.45% examples, 141799 words/s, in_qsize 9, out_qsize 0\n",
      "2017-11-09 11:47:21,305 : INFO : PROGRESS: at 97.67% examples, 141791 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:47:22,372 : INFO : PROGRESS: at 97.91% examples, 141784 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:47:23,424 : INFO : PROGRESS: at 98.14% examples, 141793 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:47:24,428 : INFO : PROGRESS: at 98.35% examples, 141775 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:47:25,469 : INFO : PROGRESS: at 98.61% examples, 141816 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:47:26,505 : INFO : PROGRESS: at 98.87% examples, 141845 words/s, in_qsize 11, out_qsize 0\n",
      "2017-11-09 11:47:27,529 : INFO : PROGRESS: at 99.09% examples, 141828 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:47:28,578 : INFO : PROGRESS: at 99.33% examples, 141838 words/s, in_qsize 9, out_qsize 0\n",
      "2017-11-09 11:47:29,569 : INFO : PROGRESS: at 99.53% examples, 141821 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:47:30,668 : INFO : PROGRESS: at 99.78% examples, 141824 words/s, in_qsize 12, out_qsize 0\n",
      "2017-11-09 11:47:31,397 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-11-09 11:47:31,469 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-11-09 11:47:31,520 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-11-09 11:47:31,549 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-09 11:47:31,572 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-09 11:47:31,592 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-09 11:47:31,592 : INFO : training on 88991350 raw words (63749129 effective words) took 449.4s, 141850 effective words/s\n",
      "2017-11-09 11:47:31,596 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-11-09 11:47:31,760 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2017-11-09 11:47:31,760 : INFO : not storing attribute syn0norm\n",
      "2017-11-09 11:47:31,765 : INFO : not storing attribute cum_table\n",
      "2017-11-09 11:47:32,551 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n",
    "\n",
    "\n",
    "#sett values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 6     # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "from gensim.models import word2vec\n",
    "print \"Training model..\"\n",
    "\n",
    "model=word2vec.Word2Vec(sentences, size=num_features, workers=num_workers, window=context,sample=downsampling,\\\n",
    "                       min_count=min_word_count)\n",
    "\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name=\"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'terrible', 0.7751983404159546),\n",
       " (u'horrible', 0.7361536026000977),\n",
       " (u'atrocious', 0.7348852157592773),\n",
       " (u'abysmal', 0.7252463102340698),\n",
       " (u'dreadful', 0.718826413154602),\n",
       " (u'horrendous', 0.6835845708847046),\n",
       " (u'horrid', 0.6716070175170898),\n",
       " (u'appalling', 0.6633854508399963),\n",
       " (u'lousy', 0.6343699097633362),\n",
       " (u'amateurish', 0.6259142756462097)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-09 11:47:32,911 : INFO : loading Word2Vec object from 300features_40minwords_10context\n",
      "2017-11-09 11:47:33,019 : INFO : loading wv recursively from 300features_40minwords_10context.wv.* with mmap=None\n",
      "2017-11-09 11:47:33,019 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-11-09 11:47:33,023 : INFO : setting ignored attribute cum_table to None\n",
      "2017-11-09 11:47:33,023 : INFO : loaded 300features_40minwords_10context\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model=Word2Vec.load(\"300features_40minwords_10context\")\n",
    "\n",
    "type(model.wv.syn0)\n",
    "\n",
    "#model.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16490L, 300L)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    featureVec=np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords=0.\n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set=set(model.wv.index2word)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords+=1\n",
    "            featureVec=np.add(featureVec,model[word])\n",
    "     \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews,model,num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    count=0\n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs=np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    \n",
    "    for review in reviews:\n",
    "        if count%1000==0:\n",
    "            print \"Review at %d of %d\"%(count,len(reviews))\n",
    "        reviewFeatureVecs[count]=makeFeatureVec(review,model,num_features)\n",
    "        \n",
    "        count+=1\n",
    "        \n",
    "    return reviewFeatureVecs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review at 0 of 25000\n",
      "Review at 1000 of 25000\n",
      "Review at 2000 of 25000\n",
      "Review at 3000 of 25000\n",
      "Review at 4000 of 25000\n",
      "Review at 5000 of 25000\n",
      "Review at 6000 of 25000\n",
      "Review at 7000 of 25000\n",
      "Review at 8000 of 25000\n",
      "Review at 9000 of 25000\n",
      "Review at 10000 of 25000\n",
      "Review at 11000 of 25000\n",
      "Review at 12000 of 25000\n",
      "Review at 13000 of 25000\n",
      "Review at 14000 of 25000\n",
      "Review at 15000 of 25000\n",
      "Review at 16000 of 25000\n",
      "Review at 17000 of 25000\n",
      "Review at 18000 of 25000\n",
      "Review at 19000 of 25000\n",
      "Review at 20000 of 25000\n",
      "Review at 21000 of 25000\n",
      "Review at 22000 of 25000\n",
      "Review at 23000 of 25000\n",
      "Review at 24000 of 25000\n",
      "Creating average feature vecs for test reviews\n",
      "Review at 0 of 25000\n",
      "Review at 1000 of 25000\n",
      "Review at 2000 of 25000\n",
      "Review at 3000 of 25000\n",
      "Review at 4000 of 25000\n",
      "Review at 5000 of 25000\n",
      "Review at 6000 of 25000\n",
      "Review at 7000 of 25000\n",
      "Review at 8000 of 25000\n",
      "Review at 9000 of 25000\n",
      "Review at 10000 of 25000\n",
      "Review at 11000 of 25000\n",
      "Review at 12000 of 25000\n",
      "Review at 13000 of 25000\n",
      "Review at 14000 of 25000\n",
      "Review at 15000 of 25000\n",
      "Review at 16000 of 25000\n",
      "Review at 17000 of 25000\n",
      "Review at 18000 of 25000\n",
      "Review at 19000 of 25000\n",
      "Review at 20000 of 25000\n",
      "Review at 21000 of 25000\n",
      "Review at 22000 of 25000\n",
      "Review at 23000 of 25000\n",
      "Review at 24000 of 25000\n"
     ]
    }
   ],
   "source": [
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. Notice that we now use stop word\n",
    "# removal.\n",
    "clean_train_reviews = []\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
    "\n",
    "print \"Creating average feature vecs for test reviews\"\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest=RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "forest=forest.fit(trainDataVecs,train[\"sentiment\"])\n",
    "\n",
    "result=forest.predict(testDataVecs)\n",
    "\n",
    "output=pd.DataFrame(data={\"id\":test[\"id\"],\"sentiment\":result})\n",
    "\n",
    "output.to_csv(\"Word2Vec_AverageVectors.csv\",quoting=3,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elaspsed time:  534.080999851\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n",
    "# average of 5 words per cluster\n",
    "word_vectors = model.wv.syn0\n",
    "num_clusters = word_vectors.shape[0] / 5\n",
    "\n",
    "kmeans=KMeans(n_clusters=num_clusters)\n",
    "idx=kmeans.fit_predict(word_vectors)\n",
    "\n",
    "end=time.time()\n",
    "elapsed=end-start\n",
    "\n",
    "print \"elaspsed time: \",elapsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number     \n",
    "word_centroid_map=dict(zip(model.wv.index2word,idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  0\n",
      "[u'maiden']\n",
      "Cluster  1\n",
      "[u'maine', u'georgia', u'scarlet', u'marsh', u'virginia']\n",
      "Cluster  2\n",
      "[u'scriptwriting', u'turgid']\n",
      "Cluster  3\n",
      "[u'argument', u'bias', u'issue', u'agenda', u'assumption']\n",
      "Cluster  4\n",
      "[u'whoa', u'freakin', u'grinning']\n",
      "Cluster  5\n",
      "[u'tempting', u'aggressive', u'threatening', u'apathetic', u'immoral', u'active', u'demanding', u'prone', u'obscene', u'unstable']\n",
      "Cluster  6\n",
      "[u'hordes', u'revolt', u'legion']\n",
      "Cluster  7\n",
      "[u'madison', u'zane', u'connolly', u'crystal', u'drago', u'thornton', u'crudup']\n",
      "Cluster  8\n",
      "[u'camerawork', u'designs', u'staging', u'costumes', u'uniformly', u'design', u'costuming']\n",
      "Cluster  9\n",
      "[u'tires', u'closeup', u'blasting', u'roar', u'doubles', u'blasts', u'belts', u'slices']\n"
     ]
    }
   ],
   "source": [
    "# For the first 10 clusters\n",
    "\n",
    "for cluster in xrange(0,10):\n",
    "    print \"Cluster \",cluster\n",
    "    \n",
    "    # Find all of the words for that cluster number, and print them out\n",
    "    words=[]\n",
    "    for i in xrange(0,len(word_centroid_map.values())):\n",
    "        if( word_centroid_map.values()[i] == cluster ):\n",
    "            words.append(word_centroid_map.keys()[i])\n",
    "    print words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    #\n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    # in the word / centroid map\n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "    #\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    #\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count \n",
    "    # by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    #\n",
    "    # Return the \"bag of centroids\"\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "train_centroids = np.zeros( (train[\"review\"].size, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "\n",
    "# Transform the training set reviews into bags of centroids\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids( review, \\\n",
    "        word_centroid_map )\n",
    "    counter += 1\n",
    "\n",
    "# Repeat for test reviews \n",
    "test_centroids = np.zeros(( test[\"review\"].size, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids( review, \\\n",
    "        word_centroid_map )\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "# Fit a random forest and extract predictions \n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# Fitting the forest may take a few minutes\n",
    "print \"Fitting a random forest to labeled training data...\"\n",
    "forest = forest.fit(train_centroids,train[\"sentiment\"])\n",
    "result = forest.predict(test_centroids)\n",
    "\n",
    "# Write the test results \n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv( \"BagOfCentroids.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
